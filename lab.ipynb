{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "062b681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VGG\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4a783",
   "metadata": {},
   "source": [
    "### ALL COMMMENTS WITH '##' ARE MINE AND NOT CHAT GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_train_cifar10_dataloader, get_test_cifar10_dataloader, run_epochs, get_hyperparams, test\n",
    "\n",
    "trainloader = get_train_cifar10_dataloader()\n",
    "testloader = get_test_cifar10_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "772546ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG(\"VGG19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de7ad2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.optim as optim\n",
    "from models import VGG\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# === Utils ===\n",
    "\n",
    "def count_nonzero_parameters(model: nn.Module):\n",
    "    return sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
    "\n",
    "## This function is that one that I did with you in the last class\n",
    "## Where we just apply the pruning to the conv2d and linear layers\n",
    "def apply_global_pruning(model: nn.Module, amount: int):\n",
    "    parameters_to_prune = []\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=amount,\n",
    "    )\n",
    "\n",
    "def remove_pruning(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'weight_mask'):\n",
    "                prune.remove(module, 'weight')\n",
    "\n",
    "# === Pruning Methods ===\n",
    "\n",
    "# 1. Global Pruning, no retrain\n",
    "## CAUE - There is nothing special here, just applying the pruning function to the model\n",
    "def method1_global_pruning_no_retrain(model, trainloader, testloader):\n",
    "    print(\"\\n=== Method 1: Global Pruning without Retrain ===\")\n",
    "    \n",
    "    apply_global_pruning(model, amount=0.3)\n",
    "\n",
    "    acc, _ = test(testloader, model)\n",
    "    \n",
    "    params = count_nonzero_parameters(model)\n",
    "    print(f\"Accuracy: {acc:.2f}% | Parameters: {params}\")\n",
    "\n",
    "    remove_pruning(model)\n",
    "\n",
    "# 2. Global Pruning + Retrain\n",
    "## CAUE - The same as the previous one, but retraining the model after the pruning\n",
    "def method2_global_pruning_with_retrain(model, trainloader, testloader):\n",
    "    print(\"\\n=== Method 2: Global Pruning with Retrain ===\")\n",
    "\n",
    "    apply_global_pruning(model, amount=0.3)\n",
    "\n",
    "    acc, _, _ = run_epochs(\n",
    "        model,\n",
    "        train_loader=trainloader,\n",
    "        test_loader=testloader,\n",
    "        hyperparams=HYPERPARAMS,\n",
    "        n_epochs=1,\n",
    "    )\n",
    "\n",
    "    params = count_nonzero_parameters(model)\n",
    "    print(f\"Accuracy after retrain: {acc:.2f}% | Parameters: {params}\")\n",
    "\n",
    "    remove_pruning(model)\n",
    "\n",
    "# 3. Gradual Pruning por etapas + Retrain\n",
    "## CAUE - What I understood from this one was that we apply the pruning many times (steps)\n",
    "## It is possible to change the pruning ratio in each step if we want\n",
    "def method3_gradual_pruning(model, trainloader, testloader):\n",
    "    print(\"\\n=== Method 3: Gradual Pruning + Retrain ===\")\n",
    "    \n",
    "    total_steps = 3\n",
    "    pruning_amount_per_step = 0.1\n",
    "\n",
    "    for step in range(total_steps):\n",
    "        print(f\"\\n-- Step {step+1}/{total_steps} --\")\n",
    "        apply_global_pruning(model, amount=pruning_amount_per_step)\n",
    "\n",
    "        acc, _, _ = run_epochs(\n",
    "            model,\n",
    "            train_loader=trainloader,\n",
    "            test_loader=testloader,\n",
    "            hyperparams=HYPERPARAMS,\n",
    "            n_epochs=1, \n",
    "        )\n",
    "\n",
    "        params = count_nonzero_parameters(model)\n",
    "        print(f\"Accuracy: {acc:.2f}% | Parameters: {params}\")\n",
    "\n",
    "    remove_pruning(model)\n",
    "\n",
    "# 4. ThiNet\n",
    "## CAUE - This one I've looked up and I found an article that describes the state of art\n",
    "## The article can be found in here: https://arxiv.org/abs/1707.06342\n",
    "## As far what I've read, it uses the L2 norm to discard the whole filter if it is considered not important\n",
    "## and not the individual weight like how we have been doing\n",
    "def method4_thinet_style_pruning(model, trainloader, testloader):\n",
    "    print(\"\\n=== Method 4: ThiNet Style Pruning ===\")\n",
    "    \n",
    "    def prune_by_feature_map_norm(model, amount=0.3):\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                weight = module.weight.detach()\n",
    "                filter_norms = weight.view(weight.size(0), -1).norm(2, dim=1)\n",
    "                num_filters_to_prune = int(amount * weight.size(0))\n",
    "                prune_idx = filter_norms.argsort()[:num_filters_to_prune]\n",
    "\n",
    "                mask = torch.ones(weight.size(0), device=weight.device)\n",
    "                mask[prune_idx] = 0\n",
    "\n",
    "                mask = mask[:, None, None, None]\n",
    "                module.weight.data.mul_(mask)\n",
    "\n",
    "    prune_by_feature_map_norm(model, amount=0.3)\n",
    "\n",
    "    acc, _, _ = run_epochs(\n",
    "        model,\n",
    "        train_loader=trainloader,\n",
    "        test_loader=testloader,\n",
    "        hyperparams=HYPERPARAMS,\n",
    "        n_epochs=1,\n",
    "    )\n",
    "\n",
    "    params = count_nonzero_parameters(model)\n",
    "    print(f\"Accuracy after ThiNet pruning + retrain: {acc:.2f}% | Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b82b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before pruning: 10.00%\n",
      "\n",
      "=== Method 1: Global Pruning without Retrain ===\n",
      "Accuracy: 10.00% | Parameters: 20035016\n",
      "\n",
      "=== Method 2: Global Pruning with Retrain ===\n",
      "Epoch: 0\n",
      "Saving..\n",
      "Accuracy after retrain: 58.93% | Parameters: 14033322\n",
      "\n",
      "=== Method 3: Gradual Pruning + Retrain ===\n",
      "\n",
      "-- Step 1/3 --\n",
      "Epoch: 0\n",
      "Saving..\n",
      "Accuracy: 71.93% | Parameters: 17954449\n",
      "\n",
      "-- Step 2/3 --\n",
      "Epoch: 0\n",
      "Saving..\n",
      "Accuracy: 74.17% | Parameters: 17954449\n",
      "\n",
      "-- Step 3/3 --\n",
      "Epoch: 0\n",
      "Saving..\n",
      "Accuracy: 80.17% | Parameters: 17954449\n",
      "\n",
      "=== Method 4: ThiNet Style Pruning ===\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m method2_global_pruning_with_retrain(model, trainloader, testloader)\n\u001b[32m     13\u001b[39m method3_gradual_pruning(model, trainloader, testloader)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mmethod4_thinet_style_pruning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mmethod4_thinet_style_pruning\u001b[39m\u001b[34m(model, trainloader, testloader)\u001b[39m\n\u001b[32m    106\u001b[39m             mask = mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    107\u001b[39m             module.weight.data.mul_(mask)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[43mprune_by_feature_map_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamount\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m acc, _, _ = run_epochs(\n\u001b[32m    112\u001b[39m     model,\n\u001b[32m    113\u001b[39m     train_loader=trainloader,\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m     n_epochs=\u001b[32m1\u001b[39m,\n\u001b[32m    117\u001b[39m )\n\u001b[32m    119\u001b[39m params = count_nonzero_parameters(model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mmethod4_thinet_style_pruning.<locals>.prune_by_feature_map_norm\u001b[39m\u001b[34m(model, amount)\u001b[39m\n\u001b[32m    104\u001b[39m mask[prune_idx] = \u001b[32m0\u001b[39m\n\u001b[32m    106\u001b[39m mask = mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "model = VGG(\"VGG19\").to(device)\n",
    "trainloader = get_train_cifar10_dataloader()\n",
    "testloader = get_test_cifar10_dataloader()\n",
    "\n",
    "## CAUE - I used your function to get the hyperparameters, but I changed the way the optimiser was defined\n",
    "## because the pickle returns it as a string\n",
    "HYPERPARAMS = get_hyperparams()\n",
    "HYPERPARAMS[\"criterion\"] = nn.CrossEntropyLoss()\n",
    "HYPERPARAMS[\"optimiser\"] = optim.AdamW(model.parameters(), lr=HYPERPARAMS['lr'], weight_decay=HYPERPARAMS['weight_decay'])\n",
    "\n",
    "acc_before, _ = test(testloader, model)\n",
    "print(f\"Accuracy before pruning: {acc_before:.2f}%\")\n",
    "\n",
    "method1_global_pruning_no_retrain(model, trainloader, testloader)\n",
    "method2_global_pruning_with_retrain(model, trainloader, testloader)\n",
    "method3_gradual_pruning(model, trainloader, testloader)\n",
    "#method4_thinet_style_pruning(model, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38fd98ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Method 4: ThiNet Style Pruning ===\n",
      "Epoch: 0\n",
      "Saving..\n",
      "Accuracy after ThiNet pruning + retrain: 79.81% | Parameters: 20019550\n"
     ]
    }
   ],
   "source": [
    "method4_thinet_style_pruning(model, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942333e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25671"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CAUE - At some point I had to use this because I got a OutOfMemoryError when running the models\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
